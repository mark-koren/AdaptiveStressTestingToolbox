{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from garage.sampler.utils import rollout\n",
    "import time\n",
    "from mylab.samplers.ast_vectorized_sampler import ASTVectorizedSampler\n",
    "from garage.tf.algos.batch_polopt import BatchPolopt\n",
    "from mylab.simulators.example_av_simulator import ExampleAVSimulator\n",
    "from mylab.rewards.example_av_reward import ExampleAVReward\n",
    "from mylab.spaces.example_av_spaces import ExampleAVSpaces\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('Loader', reuse=True):\n",
    "        data = joblib.load('./data/debug/g5/itr_1000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['baseline', 'env', 'policy', 'itr', 'paths'])\n"
     ]
    }
   ],
   "source": [
    "print(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Serializable__args',\n",
       " '_Serializable__kwargs',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_cached_assign_ops',\n",
       " '_cached_assign_placeholders',\n",
       " '_cached_param_dtypes',\n",
       " '_cached_param_shapes',\n",
       " '_cached_params',\n",
       " '_env_spec',\n",
       " '_input_layers',\n",
       " '_output_layers',\n",
       " '_serializable_initialized',\n",
       " 'action_dim',\n",
       " 'action_space',\n",
       " 'clone',\n",
       " 'dist',\n",
       " 'dist_info',\n",
       " 'dist_info_sym',\n",
       " 'distribution',\n",
       " 'env_spec',\n",
       " 'f_step_mean_std',\n",
       " 'feature_network',\n",
       " 'flat_to_params',\n",
       " 'get_action',\n",
       " 'get_actions',\n",
       " 'get_param_dtypes',\n",
       " 'get_param_shapes',\n",
       " 'get_param_values',\n",
       " 'get_params',\n",
       " 'get_params_internal',\n",
       " 'hidden_dim',\n",
       " 'input_dim',\n",
       " 'l_input',\n",
       " 'l_log_std',\n",
       " 'log_diagnostics',\n",
       " 'mean_network',\n",
       " 'observation_space',\n",
       " 'prev_actions',\n",
       " 'prev_cells',\n",
       " 'prev_hiddens',\n",
       " 'quick_init',\n",
       " 'recurrent',\n",
       " 'reset',\n",
       " 'set_param_values',\n",
       " 'state_include_action',\n",
       " 'state_info_keys',\n",
       " 'state_info_specs',\n",
       " 'terminate',\n",
       " 'vectorized']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data['policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-98d3304aea70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_param_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Research/DRL-AST/rllab/sandbox/rocky/tf/core/parameterized.py\u001b[0m in \u001b[0;36mget_param_values\u001b[0;34m(self, **tags)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_param_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mparam_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mflatten_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'run'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['itr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data['paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_action',\n",
       " '_actions',\n",
       " '_done',\n",
       " '_first_step',\n",
       " '_info',\n",
       " '_init_state',\n",
       " '_reward',\n",
       " '_sample_init_state',\n",
       " '_step',\n",
       " 'action_dim',\n",
       " 'action_only',\n",
       " 'action_space',\n",
       " 'get_cache_list',\n",
       " 'get_param_values',\n",
       " 'horizon',\n",
       " 'log',\n",
       " 'log_diagnostics',\n",
       " 'observation_space',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'reward_function',\n",
       " 'set_param_values',\n",
       " 'simulate',\n",
       " 'simulator',\n",
       " 'spaces',\n",
       " 'spec',\n",
       " 'step',\n",
       " 'terminate']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data['env'].wrapped_env.wrapped_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.791196\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('Loader', reuse=True):\n",
    "        data = joblib.load('./data/debug/d_gen7/itr_990.pkl')\n",
    "        start = time.clock()\n",
    "        for i in range(200):\n",
    "            x = rollout(data['env'],data['policy'], max_path_length=50, animated=False, speedup=1,always_return_paths=False)\n",
    "        print(time.clock() - start)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['actions', 'env_infos', 'rewards', 'agent_infos', 'observations'])\n"
     ]
    }
   ],
   "source": [
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from garage.algos import RLAlgorithm\n",
    "import garage.misc.logger as logger\n",
    "from garage.tf.plotter import Plotter\n",
    "from garage.tf.samplers import BatchSampler\n",
    "from garage.tf.samplers import OnPolicyVectorizedSampler\n",
    "\n",
    "\n",
    "class BatchPoloptCustom(RLAlgorithm):\n",
    "    \"\"\"\n",
    "    Base class for batch sampling-based policy optimization methods.\n",
    "    This includes various policy gradient methods like vpg, npg, ppo, trpo, etc.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            policy,\n",
    "            baseline,\n",
    "            scope=None,\n",
    "            n_itr=500,\n",
    "            start_itr=0,\n",
    "            batch_size=5000,\n",
    "            max_path_length=500,\n",
    "            discount=0.99,\n",
    "            gae_lambda=1,\n",
    "            plot=False,\n",
    "            pause_for_plot=False,\n",
    "            center_adv=True,\n",
    "            positive_adv=False,\n",
    "            store_paths=False,\n",
    "            whole_paths=True,\n",
    "            fixed_horizon=False,\n",
    "            sampler_cls=None,\n",
    "            sampler_args=None,\n",
    "            force_batch_sampler=False,\n",
    "            **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param env: Environment\n",
    "        :param policy: Policy\n",
    "        :type policy: Policy\n",
    "        :param baseline: Baseline\n",
    "        :param scope: Scope for identifying the algorithm. Must be specified if running multiple algorithms\n",
    "        simultaneously, each using different environments and policies\n",
    "        :param n_itr: Number of iterations.\n",
    "        :param start_itr: Starting iteration.\n",
    "        :param batch_size: Number of samples per iteration.\n",
    "        :param max_path_length: Maximum length of a single rollout.\n",
    "        :param discount: Discount.\n",
    "        :param gae_lambda: Lambda used for generalized advantage estimation.\n",
    "        :param plot: Plot evaluation run after each iteration.\n",
    "        :param pause_for_plot: Whether to pause before contiuing when plotting.\n",
    "        :param center_adv: Whether to rescale the advantages so that they have mean 0 and standard deviation 1.\n",
    "        :param positive_adv: Whether to shift the advantages so that they are always positive. When used in\n",
    "        conjunction with center_adv the advantages will be standardized before shifting.\n",
    "        :param store_paths: Whether to save all paths data to the snapshot.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.baseline = baseline\n",
    "        self.scope = scope\n",
    "        self.n_itr = n_itr\n",
    "        self.start_itr = start_itr\n",
    "        self.batch_size = batch_size\n",
    "        self.max_path_length = max_path_length\n",
    "        self.discount = discount\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.plot = plot\n",
    "        self.pause_for_plot = pause_for_plot\n",
    "        self.center_adv = center_adv\n",
    "        self.positive_adv = positive_adv\n",
    "        self.store_paths = store_paths\n",
    "        self.whole_paths = whole_paths\n",
    "        self.fixed_horizon = fixed_horizon\n",
    "        if sampler_cls is None:\n",
    "            if self.policy.vectorized and not force_batch_sampler:\n",
    "                sampler_cls = VectorizedSampler\n",
    "            else:\n",
    "                sampler_cls = BatchSampler\n",
    "        if sampler_args is None:\n",
    "            sampler_args = dict()\n",
    "        self.sampler = sampler_cls(self, **sampler_args)\n",
    "        self.init_opt()\n",
    "\n",
    "    def start_worker(self):\n",
    "        self.sampler.start_worker()\n",
    "\n",
    "    def shutdown_worker(self):\n",
    "        self.sampler.shutdown_worker()\n",
    "\n",
    "    def obtain_samples(self, itr):\n",
    "        return self.sampler.obtain_samples(itr)\n",
    "\n",
    "    def process_samples(self, itr, paths):\n",
    "        return self.sampler.process_samples(itr, paths)\n",
    "\n",
    "    def test(self, sess=None, checkpoint=None):\n",
    "        created_session = True if (sess is None) else False\n",
    "        if sess is None:\n",
    "            sess = tf.Session()\n",
    "            sess.__enter__()\n",
    "            \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        if checkpoint is not None:\n",
    "            pdb.set_trace()\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, checkpoint)\n",
    "        self.start_worker()\n",
    "        start_time = time.time()\n",
    "        itr_start_time = time.time()\n",
    "#         logger.log(\"Obtaining samples...\")\n",
    "        paths = self.obtain_samples(0)\n",
    "#         logger.log(\"Processing samples...\")\n",
    "        samples_data = self.process_samples(0, paths)\n",
    "#         logger.log(\"Logging diagnostics...\")\n",
    "#         self.log_diagnostics(paths)\n",
    "#             logger.log(\"Optimizing policy...\")\n",
    "#             self.optimize_policy(itr, samples_data)\n",
    "#             logger.log(\"Saving snapshot...\")\n",
    "#             params = self.get_itr_snapshot(itr, samples_data)  # , **kwargs)\n",
    "#         if self.store_paths:\n",
    "#             params[\"paths\"] = samples_data[\"paths\"]\n",
    "#             logger.save_itr_params(itr, params)\n",
    "#             logger.log(\"Saved\")\n",
    "#             logger.record_tabular('Time', time.time() - start_time)\n",
    "#             logger.record_tabular('ItrTime', time.time() - itr_start_time)\n",
    "#             logger.dump_tabular(with_prefix=False)\n",
    "#             if self.plot:\n",
    "#                 rollout(self.env, self.policy, animated=True, max_path_length=self.max_path_length)\n",
    "#                 if self.pause_for_plot:\n",
    "#                     input(\"Plotting evaluation run: Press Enter to \"\n",
    "#                           \"continue...\")\n",
    "        self.shutdown_worker()\n",
    "        if created_session:\n",
    "            sess.close()\n",
    "        return samples_data\n",
    "\n",
    "    def log_diagnostics(self, paths):\n",
    "        self.env.log_diagnostics(paths)\n",
    "        self.policy.log_diagnostics(paths)\n",
    "        self.baseline.log_diagnostics(paths)\n",
    "\n",
    "    def init_opt(self):\n",
    "        \"\"\"\n",
    "        Initialize the optimization procedure. If using tensorflow, this may\n",
    "        include declaring all the variables and compiling functions\n",
    "        \"\"\"\n",
    "        return None\n",
    "\n",
    "    def get_itr_snapshot(self, itr, samples_data):\n",
    "        \"\"\"\n",
    "        Returns all the data that should be saved in the snapshot for this\n",
    "        iteration.\n",
    "        \"\"\"\n",
    "        return samples_data\n",
    "\n",
    "    def optimize_policy(self, itr, samples_data):\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-12 20:41:33.468855 PST | Obtaining samples for iteration 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-12-12 20:41:54.570549 PST | fitting baseline...\n",
      "2018-12-12 20:41:54.625193 PST | fitted\n",
      "24.686548000000016\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-85233.145623913893"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler_cls = ASTVectorizedSampler\n",
    "algo = BatchPoloptCustom(\n",
    "    env=data['env'],\n",
    "    policy=data['policy'],\n",
    "    baseline=data['baseline'],\n",
    "    batch_size=50000,\n",
    "    step_size=0.0,\n",
    "    n_itr=1,\n",
    "    store_paths=True,\n",
    "    optimizer=None,\n",
    "    max_path_length=50,\n",
    "    sampler_cls=sampler_cls,\n",
    "    sampler_args={\"interactive\":False,\n",
    "                  \"sim\": data['env'].wrapped_env.wrapped_env.simulator,\n",
    "                  \"reward_function\": data['env'].wrapped_env.wrapped_env.reward_function})\n",
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('Loader', reuse=True):\n",
    "        data = joblib.load('./data/debug/g5/itr_1000.pkl')\n",
    "        start = time.clock()\n",
    "        for i in range(1):\n",
    "            x = algo.test(sess=sess)\n",
    "        print(time.clock() - start)\n",
    "max(x['rewards'][:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data['env'].reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['rewards', 'env_infos', 'valids', 'returns', 'agent_infos', 'observations', 'actions', 'advantages', 'paths'])\n"
     ]
    }
   ],
   "source": [
    "print(x.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x['paths'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.88322380728369576"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x['rewards'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'obtain_samples',\n",
       " 'process_samples',\n",
       " 'shutdown_worker',\n",
       " 'slice_dict',\n",
       " 'start_worker']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sampler_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('Loader', reuse=True):\n",
    "        data = joblib.load('./data/debug/g5/itr_1000.pkl')\n",
    "        data['policy'].reset()\n",
    "        vals = data['policy'].get_param_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-4-9fb0159c3f37>(111)test()\n",
      "-> saver = tf.train.Saver()\n",
      "(Pdb) saver = tf.train.Saver()\n",
      "(Pdb) saver.restore(sess, checkpoint)\n",
      "INFO:tensorflow:Restoring parameters from ./data/debug/g1/model.ckpt\n",
      "(Pdb) c\n",
      "INFO:tensorflow:Restoring parameters from ./data/debug/g1/model.ckpt\n",
      "2018-12-13 22:38:32 | Obtaining samples for iteration 0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:13\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.596936\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        data = joblib.load('./data/debug/g1/itr_100.pkl')\n",
    "        sampler_cls = ASTVectorizedSampler\n",
    "        algo = BatchPoloptCustom(\n",
    "        env=data['env'],\n",
    "        policy=data['policy'],\n",
    "        baseline=data['baseline'],\n",
    "        batch_size=50000,\n",
    "        step_size=0.0,\n",
    "        n_itr=1,\n",
    "        store_paths=True,\n",
    "        optimizer=None,\n",
    "        max_path_length=50,\n",
    "        sampler_cls=sampler_cls,\n",
    "        sampler_args={\"interactive\":False,\n",
    "                      \"sim\": data['env'].env.env.simulator,\n",
    "                      \"reward_function\": data['env'].env.env.reward_function})\n",
    "        start = time.clock()\n",
    "        for i in range(1):\n",
    "            x = algo.test(sess=sess, checkpoint='./data/debug/g1/model.ckpt')\n",
    "        print(time.clock() - start)\n",
    "    max(x['rewards'][:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    with tf.variable_scope('', reuse=True):\n",
    "        data = joblib.load('./data/debug/g1/itr_100.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Serializable__args',\n",
       " '_Serializable__kwargs',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_action',\n",
       " '_actions',\n",
       " '_done',\n",
       " '_first_step',\n",
       " '_info',\n",
       " '_init_state',\n",
       " '_reward',\n",
       " '_sample_init_state',\n",
       " '_serializable_initialized',\n",
       " '_step',\n",
       " 'action_only',\n",
       " 'action_space',\n",
       " 'clone',\n",
       " 'close',\n",
       " 'get_cache_list',\n",
       " 'log',\n",
       " 'log_diagnostics',\n",
       " 'metadata',\n",
       " 'observation_space',\n",
       " 'quick_init',\n",
       " 'render',\n",
       " 'reset',\n",
       " 'reward_function',\n",
       " 'reward_range',\n",
       " 'seed',\n",
       " 'simulate',\n",
       " 'simulator',\n",
       " 'spaces',\n",
       " 'spec',\n",
       " 'step',\n",
       " 'unwrapped']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(data['env'].env.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.sum(x['rewards'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 50)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['rewards'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
